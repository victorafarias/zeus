#!/bin/bash
# =====================================================
# ZEUS - Script de Instala√ß√£o do LLM Local (Ollama)
# Execute na VPS: bash setup_local_llm.sh
# =====================================================

set -e

echo "========================================"
echo "ZEUS - Instala√ß√£o do Ollama + Llama 3.1"
echo "========================================"

# Verificar se est√° rodando como root
if [ "$EUID" -ne 0 ]; then
    echo "‚ö†Ô∏è  Por favor, execute como root: sudo bash setup_local_llm.sh"
    exit 1
fi

# 1. Instalar Ollama
echo ""
echo "üì¶ Instalando Ollama..."
if command -v ollama &> /dev/null; then
    echo "‚úÖ Ollama j√° est√° instalado"
else
    curl -fsSL https://ollama.com/install.sh | sh
    echo "‚úÖ Ollama instalado com sucesso"
fi

# 2. Iniciar servi√ßo Ollama
echo ""
echo "üöÄ Iniciando servi√ßo Ollama..."
systemctl enable ollama 2>/dev/null || true
systemctl start ollama 2>/dev/null || ollama serve &

# Aguardar inicializa√ß√£o
sleep 5

# 3. Baixar modelo Llama 3.1 8B
echo ""
echo "üì• Baixando modelo Llama 3.1 8B (pode demorar alguns minutos)..."
ollama pull llama3.1:8b

# 4. Verificar instala√ß√£o
echo ""
echo "üîç Verificando instala√ß√£o..."
if ollama list | grep -q "llama3.1:8b"; then
    echo "‚úÖ Modelo llama3.1:8b instalado com sucesso!"
else
    echo "‚ùå Erro: modelo n√£o encontrado"
    exit 1
fi

# 5. Testar modelo
echo ""
echo "üß™ Testando modelo..."
RESPONSE=$(curl -s http://localhost:11434/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "llama3.1:8b",
        "messages": [{"role": "user", "content": "Responda apenas: OK"}],
        "max_tokens": 10
    }')

if echo "$RESPONSE" | grep -q "OK"; then
    echo "‚úÖ Modelo respondendo corretamente!"
else
    echo "‚ö†Ô∏è  Modelo instalado, mas teste pode ter falhado. Verifique manualmente."
fi

echo ""
echo "========================================"
echo "‚úÖ INSTALA√á√ÉO CONCLU√çDA!"
echo "========================================"
echo ""
echo "O Ollama est√° rodando em: http://localhost:11434"
echo "API compat√≠vel com OpenAI: http://localhost:11434/v1"
echo ""
echo "Para testar manualmente:"
echo "  ollama run llama3.1:8b"
echo ""
